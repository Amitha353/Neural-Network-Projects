{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian_Hyperparameter_Tuning-C2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rERTeUsYq__Q",
        "colab_type": "code",
        "outputId": "5042c6e2-9205-4b12-9f31-54be0630a9c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "import numpy as np\n",
        "from glob import glob\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pickle\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import skimage\n",
        "import sys, time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
        "from keras import Model,Sequential\n",
        "import keras\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwW47sVnrd1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/TrainDataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/Training\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkYIOZJFrpd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_ref_test = zipfile.ZipFile(\"/content/drive/My Drive/Project_C2_Testing-20191129T063942Z-001.zip\", 'r')\n",
        "zip_ref_test.extractall(\"/content/test\")\n",
        "zip_ref_test.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH9SszFTrtLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_PATH = \"/content/Training/TrainDataset\"\n",
        "TEST_PATH = \"/content/test/Project_C2_Testing\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8FxpyVLru0x",
        "colab_type": "code",
        "outputId": "d456c032-95d4-4eff-d85a-7ed45e72a004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def load_data():\n",
        "  path = TRAIN_PATH\n",
        "  df = pd.read_csv('train_files.csv')\n",
        "  X = []\n",
        "  y = []\n",
        "  # org = []\n",
        "  for i, row in df.iterrows():\n",
        "    filename = row[\"file_name\"] \n",
        "    label = row[\"annotation\"]\n",
        "    file = open('/content/Training/TrainDataset/'+filename,'rb')\n",
        "    im = cv2.imread('/content/Training/TrainDataset/'+filename)\n",
        "    X.append(im)\n",
        "    y.append(label)\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "  print(X.shape)\n",
        "  \n",
        "  return X,y\n",
        "  \n",
        "X,y = load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(896, 480, 640, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mthnCoNHrzZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX, valX, trainY, valY = train_test_split(X,y,test_size=0.3, stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD36kYqkr7iI",
        "colab_type": "code",
        "outputId": "366c6212-5b89-4af4-db56-e370126334f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "model_resnet = keras.applications.resnet.ResNet152(include_top=False, weights='imagenet',input_tensor=None, input_shape=None, pooling=None)\n",
        "output = model_resnet.output\n",
        "resnet_optimized_model = Model(input=model_resnet.input, output=output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVp9uYsIsDIh",
        "colab_type": "code",
        "outputId": "c5af5b18-f54f-412f-e9b7-5389fdeca239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "resnet_trainFeatures=resnet_optimized_model.predict(trainX)\n",
        "resnet_valFeatures=resnet_optimized_model.predict(valX)\n",
        "resnet_trainFeatures.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(627, 15, 20, 2048)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oKltMmGsMvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(resnet_trainFeatures,open('trainFeatures.pkl','wb'))\n",
        "pickle.dump(resnet_valFeatures,open('valFeatures.pkl','wb'))\n",
        "pickle.dump(trainY,open('trainLabels.pkl','wb'))\n",
        "pickle.dump(valY,open('valLabels.pkl','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFxq5Nd6swRv",
        "colab_type": "code",
        "outputId": "8cd2b92a-9fdd-464f-e181-3df3a3d8b44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "trainFeatures = pickle.load(open('trainFeatures.pkl','rb'))\n",
        "valFeatures = pickle.load(open('valFeatures.pkl','rb'))\n",
        "trainLabels = pickle.load(open('trainLabels.pkl','rb'))\n",
        "valLabels = pickle.load(open('valLabels.pkl','rb'))\n",
        "print(trainFeatures.shape,trainLabels.shape)\n",
        "print(valFeatures.shape,valLabels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(627, 15, 20, 2048) (627,)\n",
            "(269, 15, 20, 2048) (269,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaQowOmls1Ey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#parameters\n",
        "batch_size = [32,64,128,256,627]\n",
        "lr= [0.01,0.001,0.0001]\n",
        "epochs=[10,15,20,25,30,50,100,200,300,500]\n",
        "drop=[0,0.2,0.3,0.5]\n",
        "opt = ['adam','adagrad','sgd','rmsprop','adadelta']\n",
        "act = [None,'relu','tanh','sigmoid']\n",
        "hiddenunits = [64,128,256,512,1024,2048,2046,4096]\n",
        "\n",
        "param_grid = dict(lr=lr,epochs=epochs,drop=drop,batch_size=batch_size,opt=opt,act=act,hiddenunits=hiddenunits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dR5u_q0tRxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformedX = np.concatenate((trainFeatures,valFeatures))\n",
        "transformedY = np.concatenate((trainLabels,valLabels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihqCR2HqtXiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(act=None, lr=0.001, drop=0.5, opt='adam', hiddenunits=64):\n",
        "  model = Sequential()\n",
        "  model.add(GlobalAveragePooling2D(input_shape=(15, 20, 2048)))\n",
        "  if type(hiddenunits) == type(2):\n",
        "    model.add(Dense(hiddenunits, activation=act))\n",
        "    if drop > 0:\n",
        "      model.add(Dropout(drop))\n",
        "  else:\n",
        "    model.add(Dense(hiddenunits[0], activation=act))\n",
        "    if drop > 0:\n",
        "      model.add(Dropout(drop))\n",
        "    model.add(Dense(hiddenunits[1], activation=act))\n",
        "    if drop > 0:\n",
        "      model.add(Dropout(drop))\n",
        "    \n",
        "  model.add(Dense(5, activation='softmax'))\n",
        "  \n",
        "  if opt == 'adam':\n",
        "    optimizer = keras.optimizers.Adam(lr=lr)\n",
        "  elif opt == 'sgd':\n",
        "    optimizer = keras.optimizers.SGD(lr=lr)\n",
        "  elif opt == 'rmsprop':\n",
        "    optimizer = keras.optimizers.RMSprop(lr=lr)\n",
        "  elif opt == 'adadelta':\n",
        "    optimizer = keras.optimizers.Adadelta(lr=lr)\n",
        "  elif opt == 'adagrad':\n",
        "    optimizer = keras.optimizers.Adagrad(lr=lr)\n",
        "  model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paxnQqfateb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZJKZjjptj6m",
        "colab_type": "code",
        "outputId": "b845b923-d829-44f6-fa19-d6f0d19ed5ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "cv=3\n",
        "#ss = StratifiedShuffleSplit(n_splits=cv, test_size=0.3, random_state=0)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=10, cv=cv, verbose=10)  #GridSearch with 3-fold Cross Validation\n",
        "grid_result = grid.fit(transformedX, transformedY)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 96000 candidates, totalling 288000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TerminatedWorkerError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9f47f288e552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#ss = StratifiedShuffleSplit(n_splits=cv, test_size=0.3, random_state=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#GridSearch with 3-fold Cross Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformedX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformedY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    552\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGKILL(-9)}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W3NKKrYtpi9",
        "colab_type": "code",
        "outputId": "f4b54c89-feab-477a-e46d-7fe075f65efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!pip install scikit-optimize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
            "\r\u001b[K     |████▍                           | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 40kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 71kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.3.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.17.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scikit-optimize) (0.14.0)\n",
            "Installing collected packages: scikit-optimize\n",
            "Successfully installed scikit-optimize-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aL7O33rt7VG",
        "colab_type": "code",
        "outputId": "f9a2c177-2990-41df-8b91-b09d509ac266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Importing Libraries\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.python.keras import backend \n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras import optimizers\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import skopt\n",
        "from skopt import gbrt_minimize, gp_minimize\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "import gzip\n",
        "import os\n",
        "import sys\n",
        "import struct\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE-vmC6zt-Mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim_learning_rate = Real(low=1e-4, high=1e-1, prior='log-uniform',\n",
        "                         name='learning_rate')\n",
        "dim_num_dense_layers = Integer(low=1, high=3, name='num_dense_layers')\n",
        "dim_num_input_nodes = Integer(low=32, high=2048, name='num_input_nodes')\n",
        "dim_num_dense_nodes = Integer(low=32, high=1024, name='num_dense_nodes')\n",
        "dim_activation = Categorical(categories=['tanh','sigmoid','relu', 'softmax'],name='activation')\n",
        "dim_batch_size = Integer(low=64, high=128, name='batch_size')\n",
        "dim_adam_decay = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n",
        "\n",
        "dimensions = [dim_learning_rate,\n",
        "              dim_num_dense_layers,\n",
        "              dim_num_input_nodes,\n",
        "              dim_num_dense_nodes,\n",
        "              dim_activation,\n",
        "              dim_batch_size,\n",
        "              dim_adam_decay\n",
        "             ]\n",
        "default_parameters = [1e-2, 1,32, 128, 'relu',64, 1e-6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5F-w1I4uFW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model for Hyperparameter optimization\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
        "\n",
        "def build_model(learning_rate, num_dense_layers,num_input_nodes,num_dense_nodes, \n",
        "                 activation, adam_decay):\n",
        "  model = Sequential()\n",
        "  # add Convolutional layers\n",
        "  # model.add(Conv2D(filters=32, kernel_size=(3,3), activation=activation, padding='same',\n",
        "  #                    input_shape=(28,28,1)))\n",
        "  # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  # model.add(Conv2D(filters=64, kernel_size=(3,3), activation=activation, padding='same'))\n",
        "  # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  # model.add(Conv2D(filters=64, kernel_size=(3,3), activation=activation, padding='same'))\n",
        "  # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  # model.add(Flatten())\n",
        "  model.add(GlobalAveragePooling2D(input_shape=(15, 20, 2048)))\n",
        "  # Densely connected layers\n",
        "  model.add(Dense(num_input_nodes, activation=activation))\n",
        "  for i in range(num_dense_layers):\n",
        "        name = 'layer_dense_{0}'.format(i+1)\n",
        "        model.add(Dense(num_dense_nodes,\n",
        "                 activation=activation,\n",
        "                        name=name\n",
        "                 ))\n",
        "  model.add(Dense(5,activation='softmax',name=\"output_layer\"))\n",
        "  #setup our optimizer and compile\n",
        "  adam = Adam(lr=learning_rate, decay= adam_decay)\n",
        "  model.compile(optimizer = adam, loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55KvxX1wujCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform optimization\n",
        "\n",
        "@use_named_args(dimensions=dimensions)\n",
        "def fitness(learning_rate, num_dense_layers, num_input_nodes, \n",
        "            num_dense_nodes,activation, batch_size,adam_decay):\n",
        "\n",
        "    model = build_model(learning_rate=learning_rate,\n",
        "                         num_dense_layers=num_dense_layers,\n",
        "                         num_input_nodes=num_input_nodes,\n",
        "                         num_dense_nodes=num_dense_nodes,\n",
        "                         activation=activation,\n",
        "                         adam_decay=adam_decay\n",
        "                        )\n",
        "    \n",
        "\n",
        "    #named blackbox becuase it represents the structure\n",
        "#     print(trainFeatures.shape,trainLabels.shape)\n",
        "# print(valFeatures.shape,valLabels.shape)\n",
        "    blackbox = model.fit(x=trainFeatures,\n",
        "                        y=trainLabels,\n",
        "                        epochs=3,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(valFeatures, valLabels)\n",
        "                        )\n",
        "    \n",
        "    #return the validation accuracy for the last epoch.\n",
        "    count = 0\n",
        "    train_accuracy = blackbox.history['acc'][-1]\n",
        "    train_loss = blackbox.history['loss'][-1]\n",
        "    val_accuracy = blackbox.history['val_acc'][-1]\n",
        "    val_loss = blackbox.history['val_loss'][-1]\n",
        "  \n",
        "    # Print the classification accuracy.\n",
        "    print(\"Training Accuracy: {0:.2%}\".format(train_accuracy))\n",
        "    print(\"Training Loss: {0:.2%}\".format(train_loss))\n",
        "    print(\"Validation Accuracy: {0:.2%}\".format(val_accuracy))\n",
        "    print(\"Validation Loss: {0:.2%}\".format(val_loss))\n",
        "    print()\n",
        "\n",
        "\n",
        "    # Delete the Keras model with these hyper-parameters from memory.\n",
        "    del model\n",
        "    \n",
        "    # Clear the Keras session, otherwise it will keep adding new\n",
        "    # models to the same TensorFlow graph each time we create\n",
        "    # a model with a different set of hyper-parameters.\n",
        "    backend.clear_session()\n",
        "    tensorflow.reset_default_graph()\n",
        "    \n",
        "    return val_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muj18KS7unfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backend.clear_session()\n",
        "tensorflow.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEnISa0muqVU",
        "colab_type": "code",
        "outputId": "94376414-a660-44da-c275-87d1b862e3c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gp_result = gp_minimize(func=fitness,\n",
        "                            dimensions=dimensions,\n",
        "                            n_calls=30,\n",
        "                            noise= 0.01,\n",
        "                            n_jobs=-1,\n",
        "                            kappa = 5,\n",
        "                            x0=default_parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.4714 - acc: 0.7751 - val_loss: 0.7811 - val_acc: 0.6803\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.5434 - acc: 0.7352 - val_loss: 0.8797 - val_acc: 0.6171\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.4576 - acc: 0.7863 - val_loss: 0.8573 - val_acc: 0.7063\n",
            "Training Accuracy: 78.63%\n",
            "Training Loss: 45.76%\n",
            "Validation Accuracy: 70.63%\n",
            "Validation Loss: 85.73%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.4555 - acc: 0.7703 - val_loss: 0.8352 - val_acc: 0.6877\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.4121 - acc: 0.8086 - val_loss: 0.7899 - val_acc: 0.6914\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3923 - acc: 0.8309 - val_loss: 0.8236 - val_acc: 0.6914\n",
            "Training Accuracy: 83.09%\n",
            "Training Loss: 39.23%\n",
            "Validation Accuracy: 69.14%\n",
            "Validation Loss: 82.36%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.4041 - acc: 0.8262 - val_loss: 0.9393 - val_acc: 0.6654\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.4665 - acc: 0.7879 - val_loss: 1.0838 - val_acc: 0.6394\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.4655 - acc: 0.7592 - val_loss: 0.9886 - val_acc: 0.6506\n",
            "Training Accuracy: 75.92%\n",
            "Training Loss: 46.55%\n",
            "Validation Accuracy: 65.06%\n",
            "Validation Loss: 98.86%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.4294 - acc: 0.7847 - val_loss: 0.8259 - val_acc: 0.6914\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3771 - acc: 0.8246 - val_loss: 0.7444 - val_acc: 0.7063\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3421 - acc: 0.8533 - val_loss: 0.7332 - val_acc: 0.6989\n",
            "Training Accuracy: 85.33%\n",
            "Training Loss: 34.21%\n",
            "Validation Accuracy: 69.89%\n",
            "Validation Loss: 73.32%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3208 - acc: 0.8581 - val_loss: 0.7417 - val_acc: 0.7138\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3181 - acc: 0.8596 - val_loss: 0.7483 - val_acc: 0.6766\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3187 - acc: 0.8628 - val_loss: 0.7456 - val_acc: 0.7100\n",
            "Training Accuracy: 86.28%\n",
            "Training Loss: 31.87%\n",
            "Validation Accuracy: 71.00%\n",
            "Validation Loss: 74.56%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3045 - acc: 0.8628 - val_loss: 0.7670 - val_acc: 0.6989\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3172 - acc: 0.8485 - val_loss: 0.8038 - val_acc: 0.6729\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3269 - acc: 0.8437 - val_loss: 0.8207 - val_acc: 0.6766\n",
            "Training Accuracy: 84.37%\n",
            "Training Loss: 32.69%\n",
            "Validation Accuracy: 67.66%\n",
            "Validation Loss: 82.07%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3181 - acc: 0.8644 - val_loss: 0.8497 - val_acc: 0.6877\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3114 - acc: 0.8421 - val_loss: 0.9313 - val_acc: 0.7323\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3325 - acc: 0.8389 - val_loss: 0.8679 - val_acc: 0.6766\n",
            "Training Accuracy: 83.89%\n",
            "Training Loss: 33.25%\n",
            "Validation Accuracy: 67.66%\n",
            "Validation Loss: 86.79%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3712 - acc: 0.8182 - val_loss: 0.9276 - val_acc: 0.6803\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2872 - acc: 0.8772 - val_loss: 0.8388 - val_acc: 0.6803\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2785 - acc: 0.8708 - val_loss: 0.8120 - val_acc: 0.6952\n",
            "Training Accuracy: 87.08%\n",
            "Training Loss: 27.85%\n",
            "Validation Accuracy: 69.52%\n",
            "Validation Loss: 81.20%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2815 - acc: 0.8740 - val_loss: 0.8092 - val_acc: 0.6840\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3094 - acc: 0.8485 - val_loss: 0.9652 - val_acc: 0.6803\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2980 - acc: 0.8533 - val_loss: 0.8384 - val_acc: 0.6691\n",
            "Training Accuracy: 85.33%\n",
            "Training Loss: 29.80%\n",
            "Validation Accuracy: 66.91%\n",
            "Validation Loss: 83.84%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2959 - acc: 0.8549 - val_loss: 0.8698 - val_acc: 0.6803\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2673 - acc: 0.8836 - val_loss: 0.8305 - val_acc: 0.6766\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2525 - acc: 0.8868 - val_loss: 0.8469 - val_acc: 0.6914\n",
            "Training Accuracy: 88.68%\n",
            "Training Loss: 25.25%\n",
            "Validation Accuracy: 69.14%\n",
            "Validation Loss: 84.69%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2390 - acc: 0.8963 - val_loss: 0.8453 - val_acc: 0.6989\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2650 - acc: 0.8612 - val_loss: 0.8441 - val_acc: 0.6952\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2883 - acc: 0.8660 - val_loss: 1.0621 - val_acc: 0.6729\n",
            "Training Accuracy: 86.60%\n",
            "Training Loss: 28.83%\n",
            "Validation Accuracy: 67.29%\n",
            "Validation Loss: 106.21%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2933 - acc: 0.8517 - val_loss: 1.2237 - val_acc: 0.6989\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3629 - acc: 0.8134 - val_loss: 0.9743 - val_acc: 0.6766\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2539 - acc: 0.8852 - val_loss: 0.9495 - val_acc: 0.6989\n",
            "Training Accuracy: 88.52%\n",
            "Training Loss: 25.39%\n",
            "Validation Accuracy: 69.89%\n",
            "Validation Loss: 94.95%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2552 - acc: 0.8804 - val_loss: 0.9278 - val_acc: 0.6914\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2373 - acc: 0.8884 - val_loss: 0.8485 - val_acc: 0.6803\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2458 - acc: 0.8788 - val_loss: 0.8463 - val_acc: 0.7175\n",
            "Training Accuracy: 87.88%\n",
            "Training Loss: 24.58%\n",
            "Validation Accuracy: 71.75%\n",
            "Validation Loss: 84.63%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2633 - acc: 0.8724 - val_loss: 0.9496 - val_acc: 0.6691\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2305 - acc: 0.8836 - val_loss: 0.8472 - val_acc: 0.7249\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2198 - acc: 0.9091 - val_loss: 0.9080 - val_acc: 0.7063\n",
            "Training Accuracy: 90.91%\n",
            "Training Loss: 21.98%\n",
            "Validation Accuracy: 70.63%\n",
            "Validation Loss: 90.80%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 2s 3ms/step - loss: 0.2197 - acc: 0.8852 - val_loss: 0.9030 - val_acc: 0.6840\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2164 - acc: 0.8979 - val_loss: 0.9163 - val_acc: 0.7361\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2139 - acc: 0.9171 - val_loss: 0.8529 - val_acc: 0.7026\n",
            "Training Accuracy: 91.71%\n",
            "Training Loss: 21.39%\n",
            "Validation Accuracy: 70.26%\n",
            "Validation Loss: 85.29%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2683 - acc: 0.8660 - val_loss: 0.8682 - val_acc: 0.7361\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2299 - acc: 0.9011 - val_loss: 0.8349 - val_acc: 0.7286\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2883 - acc: 0.8437 - val_loss: 0.9633 - val_acc: 0.6952\n",
            "Training Accuracy: 84.37%\n",
            "Training Loss: 28.83%\n",
            "Validation Accuracy: 69.52%\n",
            "Validation Loss: 96.33%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.4228 - acc: 0.8086 - val_loss: 0.9470 - val_acc: 0.7472\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3311 - acc: 0.8453 - val_loss: 1.1086 - val_acc: 0.7361\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2643 - acc: 0.8852 - val_loss: 0.8272 - val_acc: 0.7212\n",
            "Training Accuracy: 88.52%\n",
            "Training Loss: 26.43%\n",
            "Validation Accuracy: 72.12%\n",
            "Validation Loss: 82.72%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1963 - acc: 0.9171 - val_loss: 0.8419 - val_acc: 0.7249\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1825 - acc: 0.9282 - val_loss: 0.8581 - val_acc: 0.7435\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1790 - acc: 0.9362 - val_loss: 0.9983 - val_acc: 0.7323\n",
            "Training Accuracy: 93.62%\n",
            "Training Loss: 17.90%\n",
            "Validation Accuracy: 73.23%\n",
            "Validation Loss: 99.83%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1727 - acc: 0.9474 - val_loss: 0.8750 - val_acc: 0.7323\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1766 - acc: 0.9378 - val_loss: 0.8616 - val_acc: 0.7100\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2039 - acc: 0.9250 - val_loss: 0.8241 - val_acc: 0.7249\n",
            "Training Accuracy: 92.50%\n",
            "Training Loss: 20.39%\n",
            "Validation Accuracy: 72.49%\n",
            "Validation Loss: 82.41%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1863 - acc: 0.9426 - val_loss: 0.8596 - val_acc: 0.7249\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2148 - acc: 0.9139 - val_loss: 0.8438 - val_acc: 0.7212\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1802 - acc: 0.9426 - val_loss: 0.9983 - val_acc: 0.7472\n",
            "Training Accuracy: 94.26%\n",
            "Training Loss: 18.02%\n",
            "Validation Accuracy: 74.72%\n",
            "Validation Loss: 99.83%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1468 - acc: 0.9585 - val_loss: 0.9005 - val_acc: 0.7546\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1205 - acc: 0.9665 - val_loss: 0.8872 - val_acc: 0.7361\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1187 - acc: 0.9713 - val_loss: 0.8753 - val_acc: 0.7323\n",
            "Training Accuracy: 97.13%\n",
            "Training Loss: 11.87%\n",
            "Validation Accuracy: 73.23%\n",
            "Validation Loss: 87.53%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1166 - acc: 0.9729 - val_loss: 0.8518 - val_acc: 0.7472\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1447 - acc: 0.9585 - val_loss: 0.8518 - val_acc: 0.7472\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1086 - acc: 0.9761 - val_loss: 0.9945 - val_acc: 0.7509\n",
            "Training Accuracy: 97.61%\n",
            "Training Loss: 10.86%\n",
            "Validation Accuracy: 75.09%\n",
            "Validation Loss: 99.45%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1044 - acc: 0.9761 - val_loss: 0.9684 - val_acc: 0.7472\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1044 - acc: 0.9729 - val_loss: 0.9903 - val_acc: 0.7435\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1006 - acc: 0.9761 - val_loss: 0.9373 - val_acc: 0.7323\n",
            "Training Accuracy: 97.61%\n",
            "Training Loss: 10.06%\n",
            "Validation Accuracy: 73.23%\n",
            "Validation Loss: 93.73%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1723 - acc: 0.9378 - val_loss: 0.9277 - val_acc: 0.7621\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2462 - acc: 0.8947 - val_loss: 1.0696 - val_acc: 0.7584\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3488 - acc: 0.8549 - val_loss: 1.1855 - val_acc: 0.7323\n",
            "Training Accuracy: 85.49%\n",
            "Training Loss: 34.88%\n",
            "Validation Accuracy: 73.23%\n",
            "Validation Loss: 118.55%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3188 - acc: 0.8549 - val_loss: 2.2549 - val_acc: 0.6320\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.5161 - acc: 0.8022 - val_loss: 1.0815 - val_acc: 0.7509\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.3615 - acc: 0.8485 - val_loss: 0.7936 - val_acc: 0.7732\n",
            "Training Accuracy: 84.85%\n",
            "Training Loss: 36.15%\n",
            "Validation Accuracy: 77.32%\n",
            "Validation Loss: 79.36%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2576 - acc: 0.9027 - val_loss: 1.0733 - val_acc: 0.7323\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1809 - acc: 0.9378 - val_loss: 1.1198 - val_acc: 0.6691\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.2393 - acc: 0.8947 - val_loss: 0.8409 - val_acc: 0.7398\n",
            "Training Accuracy: 89.47%\n",
            "Training Loss: 23.93%\n",
            "Validation Accuracy: 73.98%\n",
            "Validation Loss: 84.09%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1802 - acc: 0.9362 - val_loss: 1.1059 - val_acc: 0.7398\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1483 - acc: 0.9537 - val_loss: 0.9080 - val_acc: 0.7546\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1484 - acc: 0.9410 - val_loss: 0.8729 - val_acc: 0.7546\n",
            "Training Accuracy: 94.10%\n",
            "Training Loss: 14.84%\n",
            "Validation Accuracy: 75.46%\n",
            "Validation Loss: 87.29%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1037 - acc: 0.9745 - val_loss: 0.8955 - val_acc: 0.7398\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1175 - acc: 0.9681 - val_loss: 0.8837 - val_acc: 0.7435\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.1156 - acc: 0.9617 - val_loss: 0.8718 - val_acc: 0.7621\n",
            "Training Accuracy: 96.17%\n",
            "Training Loss: 11.56%\n",
            "Validation Accuracy: 76.21%\n",
            "Validation Loss: 87.18%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.0891 - acc: 0.9841 - val_loss: 0.9902 - val_acc: 0.7435\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.0842 - acc: 0.9809 - val_loss: 0.9116 - val_acc: 0.7621\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.0669 - acc: 0.9920 - val_loss: 0.9846 - val_acc: 0.7361\n",
            "Training Accuracy: 99.20%\n",
            "Training Loss: 6.69%\n",
            "Validation Accuracy: 73.61%\n",
            "Validation Loss: 98.46%\n",
            "\n",
            "Train on 627 samples, validate on 269 samples\n",
            "Epoch 1/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.0583 - acc: 0.9936 - val_loss: 1.1173 - val_acc: 0.7546\n",
            "Epoch 2/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.0635 - acc: 0.9936 - val_loss: 0.9443 - val_acc: 0.7509\n",
            "Epoch 3/3\n",
            "627/627 [==============================] - 1s 2ms/step - loss: 0.0566 - acc: 0.9936 - val_loss: 1.0622 - val_acc: 0.7584\n",
            "Training Accuracy: 99.36%\n",
            "Training Loss: 5.66%\n",
            "Validation Accuracy: 75.84%\n",
            "Validation Loss: 106.22%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvhQ7uapusKI",
        "colab_type": "code",
        "outputId": "4d51056f-b5e5-4512-fa73-32974d360545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "# Various Hyper Parameters and Accuracy achieved\n",
        "\n",
        "new_dataframe = [pd.DataFrame(gp_result.x_iters, columns = [\"learning rate\",\"hidden layers\",\"input layer nodes\",\"hidden layer nodes\",\n",
        "                                           \"activation function\",\"batch size\",\"adam learning rate decay\"]),\n",
        "(pd.Series(gp_result.func_vals*100, name=\"accuracy\"))]\n",
        "new_dataframe = pd.concat(new_dataframe,axis=1)\n",
        "new_dataframe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning rate</th>\n",
              "      <th>hidden layers</th>\n",
              "      <th>input layer nodes</th>\n",
              "      <th>hidden layer nodes</th>\n",
              "      <th>activation function</th>\n",
              "      <th>batch size</th>\n",
              "      <th>adam learning rate decay</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010000</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>70.631970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.007657</td>\n",
              "      <td>3</td>\n",
              "      <td>2021</td>\n",
              "      <td>423</td>\n",
              "      <td>relu</td>\n",
              "      <td>101</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>69.144981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.007195</td>\n",
              "      <td>2</td>\n",
              "      <td>1433</td>\n",
              "      <td>292</td>\n",
              "      <td>relu</td>\n",
              "      <td>105</td>\n",
              "      <td>0.007795</td>\n",
              "      <td>65.055764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.091027</td>\n",
              "      <td>2</td>\n",
              "      <td>78</td>\n",
              "      <td>800</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>87</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>69.888476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.033003</td>\n",
              "      <td>2</td>\n",
              "      <td>1503</td>\n",
              "      <td>581</td>\n",
              "      <td>tanh</td>\n",
              "      <td>115</td>\n",
              "      <td>0.003049</td>\n",
              "      <td>71.003718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.021218</td>\n",
              "      <td>1</td>\n",
              "      <td>598</td>\n",
              "      <td>137</td>\n",
              "      <td>tanh</td>\n",
              "      <td>103</td>\n",
              "      <td>0.007542</td>\n",
              "      <td>67.657993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.003803</td>\n",
              "      <td>1</td>\n",
              "      <td>1427</td>\n",
              "      <td>495</td>\n",
              "      <td>softmax</td>\n",
              "      <td>72</td>\n",
              "      <td>0.004895</td>\n",
              "      <td>67.657992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.044515</td>\n",
              "      <td>2</td>\n",
              "      <td>1661</td>\n",
              "      <td>974</td>\n",
              "      <td>tanh</td>\n",
              "      <td>121</td>\n",
              "      <td>0.003514</td>\n",
              "      <td>69.516729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.017247</td>\n",
              "      <td>2</td>\n",
              "      <td>238</td>\n",
              "      <td>257</td>\n",
              "      <td>relu</td>\n",
              "      <td>90</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>66.914499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.028890</td>\n",
              "      <td>2</td>\n",
              "      <td>806</td>\n",
              "      <td>248</td>\n",
              "      <td>relu</td>\n",
              "      <td>118</td>\n",
              "      <td>0.005248</td>\n",
              "      <td>69.144982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.012955</td>\n",
              "      <td>2</td>\n",
              "      <td>1265</td>\n",
              "      <td>706</td>\n",
              "      <td>tanh</td>\n",
              "      <td>73</td>\n",
              "      <td>0.001084</td>\n",
              "      <td>67.286246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000138</td>\n",
              "      <td>3</td>\n",
              "      <td>1993</td>\n",
              "      <td>859</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>65</td>\n",
              "      <td>0.009881</td>\n",
              "      <td>69.888478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000103</td>\n",
              "      <td>3</td>\n",
              "      <td>1881</td>\n",
              "      <td>54</td>\n",
              "      <td>softmax</td>\n",
              "      <td>127</td>\n",
              "      <td>0.000846</td>\n",
              "      <td>71.747214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.000156</td>\n",
              "      <td>1</td>\n",
              "      <td>54</td>\n",
              "      <td>147</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>76</td>\n",
              "      <td>0.009567</td>\n",
              "      <td>70.631970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.000100</td>\n",
              "      <td>3</td>\n",
              "      <td>2048</td>\n",
              "      <td>1024</td>\n",
              "      <td>softmax</td>\n",
              "      <td>128</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>70.260223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>1024</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>64</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>69.516729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000100</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>softmax</td>\n",
              "      <td>64</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>72.118959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.000100</td>\n",
              "      <td>1</td>\n",
              "      <td>2048</td>\n",
              "      <td>1024</td>\n",
              "      <td>tanh</td>\n",
              "      <td>128</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>73.234201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>3</td>\n",
              "      <td>2048</td>\n",
              "      <td>1024</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>128</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>72.490706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000107</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>431</td>\n",
              "      <td>softmax</td>\n",
              "      <td>65</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>74.721191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>73.234201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>1024</td>\n",
              "      <td>tanh</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>75.092937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.000100</td>\n",
              "      <td>3</td>\n",
              "      <td>2048</td>\n",
              "      <td>1024</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>128</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>73.234201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.000100</td>\n",
              "      <td>1</td>\n",
              "      <td>2048</td>\n",
              "      <td>32</td>\n",
              "      <td>relu</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>73.234201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.033978</td>\n",
              "      <td>3</td>\n",
              "      <td>1969</td>\n",
              "      <td>77</td>\n",
              "      <td>tanh</td>\n",
              "      <td>120</td>\n",
              "      <td>0.009974</td>\n",
              "      <td>77.323422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.077017</td>\n",
              "      <td>1</td>\n",
              "      <td>1999</td>\n",
              "      <td>983</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>69</td>\n",
              "      <td>0.009815</td>\n",
              "      <td>73.977695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000172</td>\n",
              "      <td>3</td>\n",
              "      <td>206</td>\n",
              "      <td>1009</td>\n",
              "      <td>relu</td>\n",
              "      <td>126</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>75.464684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.048900</td>\n",
              "      <td>3</td>\n",
              "      <td>332</td>\n",
              "      <td>65</td>\n",
              "      <td>softmax</td>\n",
              "      <td>125</td>\n",
              "      <td>0.009974</td>\n",
              "      <td>76.208178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000100</td>\n",
              "      <td>1</td>\n",
              "      <td>2048</td>\n",
              "      <td>32</td>\n",
              "      <td>softmax</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>73.605948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.000109</td>\n",
              "      <td>1</td>\n",
              "      <td>269</td>\n",
              "      <td>81</td>\n",
              "      <td>tanh</td>\n",
              "      <td>67</td>\n",
              "      <td>0.000390</td>\n",
              "      <td>75.836429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    learning rate  hidden layers  ...  adam learning rate decay   accuracy\n",
              "0        0.010000              1  ...                  0.000001  70.631970\n",
              "1        0.007657              3  ...                  0.001671  69.144981\n",
              "2        0.007195              2  ...                  0.007795  65.055764\n",
              "3        0.091027              2  ...                  0.000228  69.888476\n",
              "4        0.033003              2  ...                  0.003049  71.003718\n",
              "5        0.021218              1  ...                  0.007542  67.657993\n",
              "6        0.003803              1  ...                  0.004895  67.657992\n",
              "7        0.044515              2  ...                  0.003514  69.516729\n",
              "8        0.017247              2  ...                  0.000412  66.914499\n",
              "9        0.028890              2  ...                  0.005248  69.144982\n",
              "10       0.012955              2  ...                  0.001084  67.286246\n",
              "11       0.000138              3  ...                  0.009881  69.888478\n",
              "12       0.000103              3  ...                  0.000846  71.747214\n",
              "13       0.000156              1  ...                  0.009567  70.631970\n",
              "14       0.000100              3  ...                  0.010000  70.260223\n",
              "15       0.100000              1  ...                  0.010000  69.516729\n",
              "16       0.000100              3  ...                  0.010000  72.118959\n",
              "17       0.000100              1  ...                  0.000001  73.234201\n",
              "18       0.100000              3  ...                  0.010000  72.490706\n",
              "19       0.000107              1  ...                  0.000356  74.721191\n",
              "20       0.100000              3  ...                  0.010000  73.234201\n",
              "21       0.100000              1  ...                  0.000001  75.092937\n",
              "22       0.000100              3  ...                  0.010000  73.234201\n",
              "23       0.000100              1  ...                  0.000001  73.234201\n",
              "24       0.033978              3  ...                  0.009974  77.323422\n",
              "25       0.077017              1  ...                  0.009815  73.977695\n",
              "26       0.000172              3  ...                  0.000199  75.464684\n",
              "27       0.048900              3  ...                  0.009974  76.208178\n",
              "28       0.000100              1  ...                  0.000001  73.605948\n",
              "29       0.000109              1  ...                  0.000390  75.836429\n",
              "\n",
              "[30 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKR3QAijwIMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}